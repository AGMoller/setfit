{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b4a51f07-c477-4d27-9d66-59c1185dce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import PyTorchModelHubMixin, hf_hub_download\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import joblib\n",
    "import requests\n",
    "\n",
    "from sentence_transformers import InputExample, SentenceTransformer, losses\n",
    "from sentence_transformers.datasets import SentenceLabelDataset\n",
    "from sentence_transformers.losses.BatchHardTripletLoss import BatchHardTripletLossDistanceFunction\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from setfit.data import create_fewshot_splits\n",
    "from setfit.modeling import LOSS_NAME_TO_CLASS, SupConLoss, sentence_pairs_generation\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "from setfit.modeling import LOSS_NAME_TO_CLASS, SetFitModel, SKLearnWrapper, SupConLoss, sentence_pairs_generation\n",
    "from datasets import Dataset\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from shutil import copyfile\n",
    "from typing import Dict\n",
    "from warnings import simplefilter\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "from sentence_transformers import InputExample, SentenceTransformer, losses\n",
    "from sentence_transformers.datasets import SentenceLabelDataset\n",
    "from sentence_transformers.losses.BatchHardTripletLoss import BatchHardTripletLossDistanceFunction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from typing_extensions import LiteralString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65302fa-d39f-43ae-8e0f-119b21815a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_HEAD_NAME = \"model_head.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dac879aa-ec36-44ff-81f6-5e0b2a7f3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SetFitModel(PyTorchModelHubMixin):\n",
    "    def __init__(self, model_body=None, model_head=None):\n",
    "        super(SetFitModel, self).__init__()\n",
    "        self.model_body = model_body\n",
    "        self.model_head = model_head\n",
    "        self.model_original_state = copy.deepcopy(self.model_body.state_dict())\n",
    "\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        embeddings = self.model_body.encode(x_train)\n",
    "        self.model_head.fit(embeddings, y_train)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        embeddings = self.model_body.encode(x_test)\n",
    "        return self.model_head.predict(embeddings)\n",
    "\n",
    "    def predict_proba(self, x_test):\n",
    "        embeddings = self.model_body.encode(x_test)\n",
    "        return self.model_head.predict_proba(embeddings)\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        embeddings = self.model_body.encode(inputs)\n",
    "        return self.model_head.predict(embeddings)\n",
    "\n",
    "    def _save_pretrained(self, save_directory):\n",
    "        self.model_body.save(path=save_directory)\n",
    "        joblib.dump(self.model_head, f\"{save_directory}/{MODEL_HEAD_NAME}\")\n",
    "        \n",
    "    @classmethod\n",
    "    def _from_pretrained(\n",
    "        cls,\n",
    "        model_id,\n",
    "        revision=None,\n",
    "        cache_dir=None,\n",
    "        force_download=None,\n",
    "        proxies=None,\n",
    "        resume_download=None,\n",
    "        local_files_only=None,\n",
    "        use_auth_token=None,\n",
    "        **model_kwargs\n",
    "    ):\n",
    "        model_body = SentenceTransformer(model_id)\n",
    "        \n",
    "        if os.path.isdir(model_id) and MODEL_HEAD_NAME in os.listdir(model_id):\n",
    "            model_head_file = os.path.join(model_id, MODEL_HEAD_NAME)\n",
    "        else:\n",
    "            try:\n",
    "                model_head_file = hf_hub_download(\n",
    "                    repo_id=model_id,\n",
    "                    filename=MODEL_HEAD_NAME,\n",
    "                    revision=revision,\n",
    "                    cache_dir=cache_dir,\n",
    "                    force_download=force_download,\n",
    "                    proxies=proxies,\n",
    "                    resume_download=resume_download,\n",
    "                    use_auth_token=use_auth_token,\n",
    "                    local_files_only=local_files_only,\n",
    "                )\n",
    "            except requests.exceptions.RequestException:\n",
    "                print(f\"{MODEL_HEAD_NAME} not found on HuggingFace Hub, initialising classification head with random weights.\")\n",
    "                model_head_file = None\n",
    "\n",
    "        if model_head_file is not None:\n",
    "            model_head = joblib.load(model_head_file)\n",
    "        else:\n",
    "            model_head = LogisticRegression()\n",
    "        return SetFitModel(model_body=model_body, model_head=model_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "334780c3-808f-4a3e-b10c-ca4e2eb3eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights.\n"
     ]
    }
   ],
   "source": [
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "03354100-26c4-4c66-b9b7-10299be4b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"dummy-setfit-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8bc8aac5-becc-43d7-810e-af57e12dcc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/lewtun/dummy-setfit-model into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015768051147460938,
       "initial": 32768,
       "n": 32768,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Upload file pytorch_model.bin",
       "rate": null,
       "total": 438011953,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9228d11889745a0b36c898a5c15541e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01430511474609375,
       "initial": 324,
       "n": 324,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Upload file model_head.pkl",
       "rate": null,
       "total": 324,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5026f22f794b4fee857a4a2b8a9123be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model_head.pkl: 100%|##########| 324/324 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/lewtun/dummy-setfit-model\n",
      "   922af47..84fb8f1  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/lewtun/dummy-setfit-model/commit/84fb8f100ddeb5db789640fe893e96481b49d25c'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"dummy-setfit-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4d9b87e1-dcd8-48f2-a49e-6d89c37921d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetFitModel.from_pretrained(\"lewtun/dummy-setfit-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a22c22-372e-45df-ace6-0ec0e1a907da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "31366ebe-a845-43af-8ef0-67a6afa863a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetFitTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model = None,\n",
    "        train_dataset = None,\n",
    "        eval_dataset = None,\n",
    "        compute_metrics = None,\n",
    "        loss_class = None,\n",
    "        num_epochs = None,\n",
    "        learning_rate = None,\n",
    "        batch_size = None\n",
    "        ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.compute_metrics = compute_metrics\n",
    "        self.loss_class = loss_class\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train(self):\n",
    "        # self.model.model_body.load_state_dict(copy.deepcopy(self.model.model_original_state))\n",
    "        x_train = self.train_dataset[\"text\"]\n",
    "        y_train = self.train_dataset[\"label\"]\n",
    "\n",
    "        if self.loss_class is None:\n",
    "            return\n",
    "\n",
    "        # sentence-transformers adaptation\n",
    "        batch_size = self.batch_size\n",
    "        if self.loss_class in [\n",
    "            losses.BatchAllTripletLoss,\n",
    "            losses.BatchHardTripletLoss,\n",
    "            losses.BatchSemiHardTripletLoss,\n",
    "            losses.BatchHardSoftMarginTripletLoss,\n",
    "            SupConLoss,\n",
    "        ]:\n",
    "            train_examples = [InputExample(texts=[text], label=label) for text, label in zip(x_train, y_train)]\n",
    "            train_data_sampler = SentenceLabelDataset(train_examples)\n",
    "\n",
    "            batch_size = min(self.args.batch_size, len(train_data_sampler))\n",
    "            train_dataloader = DataLoader(train_data_sampler, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "            if self.loss_class is losses.BatchHardSoftMarginTripletLoss:\n",
    "                train_loss = self.loss_class(\n",
    "                    model=self.model,\n",
    "                    distance_metric=BatchHardTripletLossDistanceFunction.cosine_distance,\n",
    "                )\n",
    "            elif self.loss_class is SupConLoss:\n",
    "                train_loss = self.loss_class(model=self.model)\n",
    "            else:\n",
    "                train_loss = self.loss_class(\n",
    "                    model=self.model,\n",
    "                    distance_metric=BatchHardTripletLossDistanceFunction.cosine_distance,\n",
    "                    margin=0.25,\n",
    "                )\n",
    "\n",
    "            train_steps = len(train_dataloader) * self.args.num_epochs\n",
    "        else:\n",
    "            train_examples = []\n",
    "            \n",
    "            for _ in range(self.num_epochs):\n",
    "                train_examples = sentence_pairs_generation(np.array(x_train), np.array(y_train), train_examples)\n",
    "\n",
    "            train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "            train_loss = self.loss_class(self.model.model_body)\n",
    "            train_steps = len(train_dataloader)\n",
    "\n",
    "        print(f\"{len(x_train)} train samples in total, {train_steps} train steps with batch size {batch_size}\")\n",
    "\n",
    "        warmup_steps = math.ceil(train_steps * 0.1)\n",
    "        self.model.model_body.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=1,\n",
    "            steps_per_epoch=train_steps,\n",
    "            warmup_steps=warmup_steps,\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "\n",
    "        # Train the final classifier\n",
    "        self.model.fit(x_train, y_train)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Computes the metrics for a given classifier.\"\"\"\n",
    "        # Define metrics\n",
    "        metric_fn = load(\"accuracy\")\n",
    "\n",
    "        x_test = self.eval_dataset[\"text\"]\n",
    "        y_test = self.eval_dataset[\"label\"]\n",
    "\n",
    "        y_pred = self.model.predict(x_test)\n",
    "\n",
    "        metrics = metric_fn.compute(predictions=y_pred, references=y_test)\n",
    "        print(f\"{metrics}\")\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de749362-357b-4b73-92cd-0b9adaf4fd6a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7d6557e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"SetFit/sst2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b667a84-dc86-430c-8ec3-08491a9e4386",
   "metadata": {},
   "source": [
    "##### We load the \"train\" and \"test\" portions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "92294e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--sst2-4811211b52125821\n",
      "Reusing dataset json (/home/lewis_huggingface_co/.cache/huggingface/datasets/SetFit___json/SetFit--sst2-4811211b52125821/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014069557189941406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 26,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbb0e4c8b604dad9f089b1de48b7d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sst2_data = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5e1c360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sst2 = sst2_data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1a5d9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sst2 = sst2_data[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee4f38-f666-4110-854e-b406f5ecffd8",
   "metadata": {},
   "source": [
    "##### Let's now import this fewshot split generating function for the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50657ab-54fa-4670-9561-596bf002f4ad",
   "metadata": {},
   "source": [
    "##### We not sample our data so that we have n number of examples for each class. We start with 16 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d0901752",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8\n",
    "fewshot_sst2 = create_fewshot_splits(train_sst2, [n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da4389-aac1-45b5-9027-26a5c868abac",
   "metadata": {},
   "source": [
    "##### Create_fewshot_splits has samples 10 different groups of n=16 (per class) data splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37605aee-8f5e-497d-b8c8-957d491dd222",
   "metadata": {},
   "source": [
    "##### Let's try our SetFit test on just one run. We'll call it try1. This means we're training our model on just one run of 16 examples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "23f8c3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'label_text'],\n",
       "    num_rows: 16\n",
       "})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try1 = 'train-8-0'\n",
    "fewshot_sst2[try1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c474cb6d-cbaa-4bf9-9204-5b3244266812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights.\n"
     ]
    }
   ],
   "source": [
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "trainer = SetFitTrainer(model=model, train_dataset=fewshot_sst2[try1], eval_dataset=test_sst2, loss_class=losses.CosineSimilarityLoss, batch_size=16, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e4ac1e34-6c13-466f-a492-e0d5d1afa1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 train samples in total, 40 train steps with batch size 16\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014457225799560547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 26,
       "postfix": null,
       "prefix": "Epoch",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd76a975f92e40edae7dbfeefaa95dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021229267120361328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 26,
       "postfix": null,
       "prefix": "Iteration",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43168be91944ca090978b4fc32607f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "39bfd86d-6e95-4020-be2c-5e8decf6848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8841295991213619}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8841295991213619}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bb314-d34e-4121-a5a0-96196102af3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
