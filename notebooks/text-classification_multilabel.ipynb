{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552ec552",
   "metadata": {},
   "source": [
    "# SetFit for Multilabel Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250e1a6",
   "metadata": {},
   "source": [
    "Welcome to SetFit for multilabel text classification! In this short notebook exercise we'll go into just how easy it is to do few-shot classification with SetFit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4ce45",
   "metadata": {},
   "source": [
    "To start, we install SetFit which will install all of the dependencies you will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8c41a",
   "metadata": {},
   "source": [
    "We then import relevant functions (data loading and loss function) and our handy SetFitModel and SetFitTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44b7069-27b0-49ea-bc27-c44f94a98e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelepangrazzi/workspace/setfit/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3e642",
   "metadata": {},
   "source": [
    "We start by loading a dataset to work with. In this example we'll load the \"emotion\" dataset already on the HF hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160d1a9a-11e3-4df6-8861-e21a17a2911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 3.62kB [00:00, 577kB/s]                    \n",
      "Downloading metadata: 3.28kB [00:00, 8.62kB/s]                   \n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to /Users/michelepangrazzi/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.66M/1.66M [00:00<00:00, 3.30MB/s]\n",
      "Downloading data: 100%|██████████| 204k/204k [00:00<00:00, 383kB/s] \n",
      "Downloading data: 100%|██████████| 207k/207k [00:00<00:00, 1.63MB/s]\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset emotion downloaded and prepared to /Users/michelepangrazzi/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 615.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset\n",
    "dataset = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f2ee2",
   "metadata": {},
   "source": [
    "Let's now select N number of examples per class in our train and test sets. In our example let's start with N=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126e1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select N examples per class (8 in this case)\n",
    "train_ds = dataset[\"train\"].shuffle(seed=42).select(range(8 * 2))\n",
    "test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b908232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'while cycling in the country', 'label': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042168df",
   "metadata": {},
   "source": [
    "Now load a Sentence Transformer model, also accessible from the HF hub. In this example we download paraphrase-mpnet. You can use the SetFitModel function which handles the SetFit component of the downloaded model for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86829321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights.\n"
     ]
    }
   ],
   "source": [
    "# Load SetFit model from Hub\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\", multi_target_strategy=\"one-vs-rest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a637650",
   "metadata": {},
   "source": [
    "Now we're ready to train! Again, the SetFitTrainer function we have provided makes this easy! Input the 1. model, 2. train/test sets we've generated above, 3. loss function we've imported, 4. batch size, and 5. number of epochs/iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c353ee-b825-40f2-86ca-cf2c6698493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=16,\n",
    "    num_epochs=1,\n",
    "    num_iterations=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e5027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!echo CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cf6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799f994",
   "metadata": {},
   "source": [
    "Now you can check your model's performance with the trainer's evaluate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6828f3",
   "metadata": {},
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982b96c",
   "metadata": {},
   "source": [
    "If you'd like to upload your newly SetFit few-shot trained model to the hub you can log in via cli to the HF hub..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80667bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553aeb9",
   "metadata": {},
   "source": [
    "... then push to hub! Success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420c4b9-1552-45a5-888c-cdbb78f8e4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Push model to the Hub\n",
    "trainer.push_to_hub(\"my-awesome-setfit-model-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d6f29",
   "metadata": {},
   "source": [
    "With our implementation of SetFit, downloading, training, and uploading it back to the hub can be done in just a few lines of code :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb3015ee729ce45d8c3dbdde1cbdfc16cc594d9f92f105c47bb1f516f6d07ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
